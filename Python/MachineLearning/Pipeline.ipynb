{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f1fab1-f15a-427a-8ab3-829d0f8a742d",
   "metadata": {},
   "source": [
    "# Data Cleaning (Numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6005b8-9cd2-4be0-961c-445c1f06c023",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "45.65077198892804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "## Loading the dataset\n",
    "columns = [\"sex\",\"length\",\"diam\",\"height\",\"whole\",\"shucked\",\"viscera\",\"shell\",\"age\"]\n",
    "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\",names=columns)\n",
    "## Defining target and predictor variables\n",
    "y = df.age\n",
    "X = df.drop(columns=['age'])\n",
    "\n",
    "## Numerical columns:\n",
    "num_cols = X.select_dtypes(include=np.number).columns\n",
    "## Categorical columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "## Create some missing values\n",
    "for i in range(1000):\n",
    "    X.loc[np.random.choice(X.index),np.random.choice(X.columns)] = np.nan\n",
    "\n",
    "## Perform train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "#####-------Imputation and Scaling: Code base to transform -----------------#####\n",
    "## Numerical training data\n",
    "x_train_num = x_train[num_cols]\n",
    "# Filling in missing values with mean on numeric features only\n",
    "x_train_fill_missing = x_train_num.fillna(x_train_num.mean())\n",
    "## Fitting standard scaler on x_train_fill_missing\n",
    "scale = StandardScaler().fit(x_train_fill_missing)\n",
    "## Scaling data after filling in missing values\n",
    "x_train_fill_missing_scale = scale.transform(x_train_fill_missing)\n",
    "## Same steps as above, but on the test set:\n",
    "x_test_fill_missing = x_test[num_cols].fillna(x_train_num.mean())\n",
    "x_test_fill_missing_scale = scale.transform(x_test_fill_missing)\n",
    "#####-------Imputation and Scaling: Code base to transform -----------------#####\n",
    "\n",
    "#1. Rewrite using Pipelines!\n",
    "pipeline = Pipeline([(\"imputer\",SimpleImputer(strategy='mean')), (\"scale\",StandardScaler())])\n",
    "\n",
    "#2. Fit pipeline on the test and compare results\n",
    "pipeline.fit(x_train[num_cols])\n",
    "x_transform = pipeline.transform(x_test[num_cols])\n",
    "\n",
    "# 3. Verify pipeline transform test set is the same by using np.array_equal()\n",
    "array_diff= np.array_equal(x_transform,x_test_fill_missing_scale)\n",
    "print(array_diff)\n",
    "\n",
    "#4. Change imputer strategy to median\n",
    "pipeline_median =Pipeline([(\"imputer\",SimpleImputer(strategy='median')), (\"scale\",StandardScaler())])\n",
    "pipeline_median.fit(x_train[num_cols])\n",
    "\n",
    "# 5 Compare results between the two pipelines\n",
    "x_transform_median = pipeline_median.transform(x_test[num_cols])\n",
    "new_array_diff = abs(x_transform-x_transform_median).sum()\n",
    "print(new_array_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fa6b70-61d4-44bc-adc9-e7fd10e1209a",
   "metadata": {},
   "source": [
    "# Data Cleaning (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a87543e-34d1-49d5-9e7a-5aa8108bd3af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the arrays equal?\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benson/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/benson/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "columns = [\"sex\",\"length\",\"diam\",\"height\",\"whole\",\"shucked\",\"viscera\",\"shell\",\"age\"]\n",
    "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\",names=columns)\n",
    "\n",
    "y = df.age\n",
    "X=df.drop(columns=['age'])\n",
    "num_cols = X.select_dtypes(include=np.number).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "#create some missing values\n",
    "for i in range(1000):\n",
    "    X.loc[np.random.choice(X.index),np.random.choice(X.columns)] = np.nan\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "x_train_cat = x_train[cat_cols]\n",
    "#fill missing values with mode on categorical features only\n",
    "x_train_fill_missing = x_train_cat.fillna(x_train_cat.mode().values[0][0])\n",
    "#apply one hot encoding on x_train_fill_missing\n",
    "ohe = OneHotEncoder(sparse=False, drop='first').fit(x_train_fill_missing)\n",
    "#transform data after filling in missing values\n",
    "x_train_fill_missing_ohe = ohe.transform(x_train_fill_missing)\n",
    "\n",
    "#Now want to do the same thing on the test set! \n",
    "x_test_fill_missing = x_test[cat_cols].fillna(x_train_cat.mode().values[0][0])\n",
    "x_test_fill_missing_ohe = ohe.transform(x_test_fill_missing)\n",
    "\n",
    "#1. Rewrite using Pipelines!\n",
    "pipeline = Pipeline([(\"imputer\",SimpleImputer(strategy='most_frequent')), (\"ohe\",OneHotEncoder(sparse=False, drop='first'))])\n",
    "\n",
    "\n",
    "#2. Fit the pipeline and transform the test data (categorical columns only!)\n",
    "pipeline.fit(x_train[cat_cols])\n",
    "x_transform = pipeline.transform(x_test[cat_cols])\n",
    "\n",
    "#3. Check if the two arrays are the same using np.array_equal()\n",
    "check_arrays = np.array_equal(x_transform,x_test_fill_missing_ohe)\n",
    "print('Are the arrays equal?')\n",
    "print(check_arrays)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6cd86-0209-4b06-8703-7e0b99bddc28",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Column Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9592fe02-2ee3-49bc-9767-6ea734348d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benson/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "columns = [\"sex\",\"length\",\"diam\",\"height\",\"whole\",\"shucked\",\"viscera\",\"shell\",\"age\"]\n",
    "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\",names=columns)\n",
    "\n",
    "y = df.age\n",
    "X=df.drop(columns=['age'])\n",
    "num_cols = X.select_dtypes(include=np.number).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "#create some missing values\n",
    "for i in range(1000):\n",
    "    X.loc[np.random.choice(X.index),np.random.choice(X.columns)] = np.nan\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "#1. Create a pipeline `num_vals` to process numerical data\n",
    "\n",
    "num_vals = Pipeline([(\"imputer\",SimpleImputer()), (\"scale\",StandardScaler())])\n",
    "\n",
    "#2. Create a pipeline `cat_vals` to process categorical data\n",
    "cat_vals = Pipeline([(\"imputer\",SimpleImputer(strategy = 'most_frequent')), (\"ohe\",OneHotEncoder(drop = 'first', sparse = False))])\n",
    "\n",
    "\n",
    "#3. Create a column transformer, `preprocess` with the numerical and categorical pipelines\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_preprocess\", num_vals, num_cols),\n",
    "        (\"cat_preprocess\", cat_vals, cat_cols)      \n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "#4. Fit the preprocess transformer to training data\n",
    "preprocess.fit(x_train)\n",
    "#Transform the test data\n",
    "x_transform = preprocess.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c3b50-5942-4658-9549-4701ba2b65f6",
   "metadata": {},
   "source": [
    "# Adding a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da7cf880-ad95-4d70-a1db-beec574b7700",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5074165965040656\n",
      "0.5074165965040656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benson/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "columns = [\"sex\",\"length\",\"diam\",\"height\",\"whole\",\"shucked\",\"viscera\",\"shell\",\"age\"]\n",
    "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\",names=columns)\n",
    "\n",
    "y = df.age\n",
    "X=df.drop(columns=['age'])\n",
    "num_cols = X.select_dtypes(include=np.number).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "#create some missing values\n",
    "for i in range(1000):\n",
    "    X.loc[np.random.choice(X.index),np.random.choice(X.columns)] = np.nan\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "cat_vals = Pipeline([(\"imputer\",SimpleImputer(strategy='most_frequent')), (\"ohe\",OneHotEncoder(sparse=False, drop='first'))])\n",
    "\n",
    "num_vals = Pipeline([(\"imputer\",SimpleImputer(strategy='mean')), (\"scale\",StandardScaler())])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat_process\", cat_vals, cat_cols),\n",
    "        (\"num_process\", num_vals, num_cols)\n",
    "    ]\n",
    ")\n",
    "#1. Create a pipeline with `preprocess` and a linear regression model, `regr`\n",
    "pipeline = Pipeline([(\"preprocess\",preprocess), \n",
    "                     (\"regr\",LinearRegression())])\n",
    "\n",
    "#2. Fit the pipeline on the training data and predict on the test data\n",
    "\n",
    "pipeline.fit(x_train, y_train)\n",
    "y_pred = pipeline.predict(x_test)\n",
    "\n",
    "#3. Calculate pipeline score and compare to estimator score\n",
    "#Pipeline score\n",
    "pipeline_score = pipeline.score(x_test,y_test)\n",
    "print(pipeline_score)\n",
    "\n",
    "#r-squared score\n",
    "r2_score = r2_score(y_test, y_pred)\n",
    "print(r2_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b02693e-d9b6-4695-b1c8-018c8b13a8a1",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a2892-558f-4446-b7e6-2ca01286a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import metrics\n",
    "\n",
    "columns = [\"sex\",\"length\",\"diam\",\"height\",\"whole\",\"shucked\",\"viscera\",\"shell\",\"age\"]\n",
    "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\",names=columns)\n",
    "\n",
    "y = df.age\n",
    "X=df.drop(columns=['age'])\n",
    "num_cols = X.select_dtypes(include=np.number).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "#create some missing values\n",
    "for i in range(1000):\n",
    "    X.loc[np.random.choice(X.index),np.random.choice(X.columns)] = np.nan\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "cat_vals = Pipeline([(\"imputer\",SimpleImputer(strategy='most_frequent')), (\"ohe\",OneHotEncoder(sparse=False, drop='first'))])\n",
    "num_vals = Pipeline([(\"imputer\",SimpleImputer(strategy='mean')), (\"scale\",StandardScaler())])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat_process\", cat_vals, cat_cols),\n",
    "        (\"num_process\", num_vals, num_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Create a pipeline with pregrocess and a linear regression model\n",
    "pipeline = Pipeline([(\"preprocess\",preprocess), \n",
    "                     (\"regr\",LinearRegression())])\n",
    "\n",
    "#Very simple parameter grid, with and without the intercept\n",
    "param_grid = {\n",
    "    \"regr__fit_intercept\": [True,False]\n",
    "}\n",
    "\n",
    "#------------------------------------------------\n",
    "#1. Grid search using previous pipeline\n",
    "gs = GridSearchCV(pipeline, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "#2. fit grid using training data and print best score\n",
    "gs.fit(x_train, y_train)\n",
    "best_score = gs.best_score_\n",
    "best_params = gs.best_params_\n",
    "print(best_score, best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd405355-5ce4-436d-836c-b5e468c33054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import metrics\n",
    "\n",
    "columns = [\"sex\",\"length\",\"diam\",\"height\",\"whole\",\"shucked\",\"viscera\",\"shell\",\"age\"]\n",
    "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\",names=columns)\n",
    "\n",
    "y = df.age\n",
    "X=df.drop(columns=['age'])\n",
    "num_cols = X.select_dtypes(include=np.number).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "#create some missing values\n",
    "for i in range(1000):\n",
    "    X.loc[np.random.choice(X.index),np.random.choice(X.columns)] = np.nan\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "cat_vals = Pipeline([(\"imputer\",SimpleImputer(strategy='most_frequent')), (\"ohe\",OneHotEncoder(sparse=False, drop='first'))])\n",
    "num_vals = Pipeline([(\"imputer\",SimpleImputer(strategy='mean')), (\"scale\",StandardScaler())])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat_preprocess\", cat_vals, cat_cols),\n",
    "        (\"num_preprocess\", num_vals, num_cols)\n",
    "    ]\n",
    ")\n",
    "#Create a pipeline with preprocess and a linear regression model\n",
    "pipeline = Pipeline([(\"preprocess\",preprocess), \n",
    "                     (\"regr\",LinearRegression())])\n",
    "\n",
    "#--------------------------------------\n",
    "# 1. Update the `search_space` array from the narrative to add a Lasso Regression model as the third dictionary.\n",
    "search_space = [{'regr': [LinearRegression()], 'regr__fit_intercept': [True,False]},\n",
    "                {'regr':[Ridge()],\n",
    "                     'regr__alpha': [0.01,0.1,1,10,100]},\n",
    "                {'regr':[Lasso()],\n",
    "                     'regr__alpha': [0.01,0.1,1,10,100]}]\n",
    "\n",
    "\n",
    "# 2.  Initialize a grid search on `search_space`\n",
    "gs = GridSearchCV(pipeline, search_space, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "#3. Find the best pipeline, regression model and its hyperparameters\n",
    "\n",
    "## i. Fit to training data\n",
    "gs.fit(x_train, y_train)\n",
    "\n",
    "## ii. Find the best pipeline\n",
    "best_pipeline = gs.best_estimator_\n",
    "\n",
    "## iii. Find the best regression model\n",
    "best_regression_model = best_pipeline.named_steps['regr']\n",
    "print('The best regression model is:')\n",
    "print(best_regression_model)\n",
    "\n",
    "## iv. Find the hyperparameters of the best regression model\n",
    "best_model_hyperparameters = best_regression_model.get_params()\n",
    "print('The hyperparameters of the regression model are:')\n",
    "print(best_model_hyperparameters)\n",
    "\n",
    "#4. Access the hyperparameters of the categorical preprocessing step\n",
    "cat_preprocess_hyperparameters = best_pipeline.named_steps['preprocess'].named_transformers_['cat_preprocess'].named_steps['imputer'].get_params()\n",
    "print('The hyperparameters of the imputer are:')\n",
    "print(cat_preprocess_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c611b-055c-4c9a-ac5f-63f54e4909b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "columns = [\"sex\",\"length\",\"diam\",\"height\",\"whole\",\"shucked\",\"viscera\",\"shell\",\"age\"]\n",
    "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\",names=columns)\n",
    "y = df.age\n",
    "X=df.drop(columns=['age'])\n",
    "num_cols = X.select_dtypes(include=np.number).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "for i in range(1000):\n",
    "    X.loc[np.random.choice(X.index),np.random.choice(X.columns)] = np.nan\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "x_train_num = x_train[num_cols]\n",
    "#fill missing values with mean on numeric features only\n",
    "x_train_fill_missing = x_train_num.fillna(x_train_num.mean())\n",
    "#fit standard scaler on x_train_fill_missing\n",
    "scale = StandardScaler().fit(x_train_fill_missing)\n",
    "#scale data after filling in missing values\n",
    "x_train_fill_missing_scale = scale.transform(x_train_fill_missing)\n",
    "x_test_fill_missing = x_test[num_cols].fillna(x_train_num.mean())\n",
    "x_test_fill_missing_scale = scale.transform(x_test_fill_missing)\n",
    "\n",
    "class MyImputer(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        self.means = np.mean(X, axis=0)    # calculate the mean of each column\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        #transform method fills in missing values with means using pandas\n",
    "        return X.fillna(self.means)\n",
    "\n",
    "#1. Create new pipeline using the custom class MyImputer as the first step and standard scaler on the second\n",
    "new_pipeline = Pipeline([(\"imputer\",MyImputer()), (\"scale\",StandardScaler())])\n",
    "\n",
    "#2. 1.Fit new pipeline on the training data with num_cols only\n",
    "new_pipeline.fit(x_train[num_cols])\n",
    "x_transform = new_pipeline.transform(x_test[num_cols])\n",
    "\n",
    "#2 2. Verify that the results of the transform are the same on test set\n",
    "check_arrays = np.array_equal(x_transform, x_test_fill_missing_scale)\n",
    "print(check_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18321d6f-2210-4795-9927-1fc0d1e4ca48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
